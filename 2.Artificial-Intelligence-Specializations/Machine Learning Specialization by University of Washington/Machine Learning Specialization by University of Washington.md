# [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning) `100H`

## SKILLS YOU WILL GAIN
`data clustering algorithms` `machine learning` `classification algorithms` `decision tree` `python programming` `machine learning concepts` `deep learning` `linear regression` `ridge regression` `lasso (statistics)` `regression analysis` `logistic regression` `statistical classification` `k-means clustering` `k-d tree`

## About this Specialization
This Specialization from leading researchers at the University of Washington introduces you to the exciting, high-demand field of Machine Learning. Through a series of practical case studies, you will gain applied experience in major areas of Machine Learning including Prediction, Classification, Clustering, and Information Retrieval. You will learn to analyze large and complex datasets, create systems that adapt and improve over time, and build intelligent applications that can make predictions from data.

## Applied Learning Project
Learners will implement and apply predictive, classification, clustering, and information retrieval machine learning algorithms to real datasets throughout each course in the specialization. They will walk away with applied machine learning and Python programming experience.

<details>
	<summary>Specialization Details</summary>

- Do you have data and wonder what it can tell you? 
- Do you need a deeper understanding of the core ways in which machine learning can improve your business? 
- Do you want to be able to converse with specialists about anything from regression and classification to deep learning and recommender systems?

- In the first course, you will get hands-on experience with machine learning from a series of practical case-studies. At the end of the first course you will have studied how to predict house prices based on house-level features, analyze sentiment from user reviews, retrieve documents of interest, recommend products, and search for images. 
- Through hands-on practice with these use cases, you will be able to apply machine learning methods in a wide range of domains. This first course treats the machine learning method as a black box. Using this abstraction, you will focus on understanding tasks of interest, matching these tasks to machine learning tools, and assessing the quality of the output. In subsequent courses, you will delve into the components of this black box by examining models and algorithms. Together, these pieces form the machine learning pipeline, which you will use in developing intelligent applications. Learning Outcomes: By the end of this course, you will be able to: -Identify potential applications of machine learning in practice. -Describe the core differences in analyses enabled by regression, classification, and clustering. -Select the appropriate machine learning task for a potential application. -Apply regression, classification, clustering, retrieval, recommender systems, and deep learning. -Represent your data as features to serve as input to machine learning models. -Assess the model quality in terms of relevant error metrics for each task. -Utilize a dataset to fit a model to analyze new data. -Build an end-to-end application that uses machine learning at its core. -Implement these techniques in Python.

- In our first case study (Predicting Housing Prices), predicting house prices, you will create models that predict a continuous value (price) from input features (square footage, number of bedrooms and bathrooms,...). This is just one of the many places where regression can be applied. Other applications range from predicting health outcomes in medicine, stock prices in finance, and power usage in high-performance computing, to analyzing which regulators are important for gene expression. 
- In this course, you will explore regularized linear regression models for the task of prediction and feature selection. You will be able to handle very large sets of features and select between models of various complexity. 
- You will also analyze the impact of aspects of your data -- such as outliers -- on your selected models and predictions. To fit these models, you will implement optimization algorithms that scale to large datasets. Learning Outcomes: By the end of this course, you will be able to: 
  - Describe the input and output of a regression model. 
  - Compare and contrast bias and variance when modeling data. 
  - Estimate model parameters using optimization algorithms. 
  - Tune parameters with cross validation. 
  - Analyze the performance of the model. 
  - Describe the notion of sparsity and how LASSO leads to sparse solutions. 
  - Deploy methods to select between models. 
  - Exploit the model to form predictions. 
  - Build a regression model to predict prices using a housing dataset. 
  - Implement these techniques in Python.

- In our second case study (Analyzing Sentiment & Loan Default Prediction) on analyzing sentiment, you will create models that predict a class (positive/negative sentiment) from input features (text of the reviews, user profile information,...). In our second case study for this course, loan default prediction, you will tackle financial data, and predict when a loan is likely to be risky or safe for the bank. These tasks are an examples of classification, one of the most widely used areas of machine learning, with a broad array of applications, including ad targeting, spam detection, medical diagnosis and image classification. 
- In this course, you will create classifiers that provide state-of-the-art performance on a variety of tasks. You will become familiar with the most successful techniques, which are most widely used in practice, including logistic regression, decision trees and boosting. 
- In addition, you will be able to design and implement the underlying algorithms that can learn these models at scale, using stochastic gradient ascent. You will implement these technique on real-world, large-scale machine learning tasks. You will also address significant tasks you will face in real-world applications of ML, including handling missing data and measuring precision and recall to evaluate a classifier. This course is hands-on, action-packed, and full of visualizations and illustrations of how these techniques will behave on real data. We've also included optional content in every module, covering advanced topics for those who want to go even deeper! Learning Objectives: By the end of this course, you will be able to: 
  - Describe the input and output of a classification model. 
  - Tackle both binary and multiclass classification problems. 
  - Implement a logistic regression model for large-scale classification. 
  - Create a non-linear model using decision trees. 
  - Improve the performance of any model using boosting. 
  - Scale your methods with stochastic gradient ascent. 
  - Describe the underlying decision boundaries. 
  - Build a classification model to predict sentiment in a product review dataset. 
  - Analyze financial data to predict loan defaults. 
  - Use techniques for handling missing data. 
  - Evaluate your models using precision-recall metrics. 
  - Implement these techniques in Python (or in the language of your choice, though Python is highly recommended).

- A reader is interested in a specific news article and you want to find similar articles to recommend. What is the right notion of similarity? Moreover, what if there are millions of other documents? Each time you want to a retrieve a new document, do you need to search through all other documents? How do you group similar documents together? How do you discover new, emerging topics that the documents cover? 
- In this third case study (Finding Similar Documents), finding similar documents, you will examine similarity-based algorithms for retrieval. In this course, you will also examine structured representations for describing the documents in the corpus, including clustering and mixed membership models, such as latent Dirichlet allocation (LDA). You will implement expectation maximization (EM) to learn the document clusterings, and see how to scale the methods using MapReduce. Learning Outcomes: By the end of this course, you will be able to: 
  - Create a document retrieval system using k-nearest neighbors. 
  - Identify various similarity metrics for text data. 
  - Reduce computations in k-nearest neighbor search by using KD-trees. 
  - Produce approximate nearest neighbors using locality sensitive hashing. 
  - Compare and contrast supervised and unsupervised learning tasks. 
  - Cluster documents by topic using k-means. 
  - Describe how to parallelize k-means using MapReduce. 
  - Examine probabilistic clustering approaches using mixtures models. 
  - Fit a mixture of Gaussian model using expectation maximization (EM). 
  - Perform mixed membership modeling using latent Dirichlet allocation (LDA). 
  - Describe the steps of a Gibbs sampler and how to use its output to draw inferences. 
  - Compare and contrast initialization techniques for non-convex optimization objectives. 
  - Implement these techniques in Python.

</details>

## There are 4 Courses in this Specialization

## Course 1: [Machine Learning Foundations: A Case Study Approach](https://www.coursera.org/learn/ml-foundations) `25H`

### Week 1: Welcome
- Why you should learn machine learning with us
- Who this specialization is for and what you will be able to do
- Getting started with the tools for the course
- Getting started with Python and the IPython Notebook
- Getting started with SFrames for data engineering and analysis

### Week 2: Regression: Predicting House Prices
- Linear regression modeling
- Evaluating regression models
- Summary of regression
- Predicting house prices: IPython Notebook
- Programming assignment

### Week 3: Classification: Analyzing Sentiment
- Classification modeling
- Evaluating classification models
- Summary of classification
- Analyzing sentiment: IPython Notebook
- Programming assignment

### Week 4: Clustering and Similarity: Retrieving Documents
- Algorithms for retrieval and measuring similarity of documents
- Clustering models and algorithms
- Summary of clustering and similarity
- Document retrieval: IPython Notebook
- Programming assignment

### Week 5: Recommending Products
- Recommender systems
- Co-occurrence matrices for collaborative filtering
- Matrix factorization
- Performance metrics for recommender systems
- Summary of recommender systems
- Song recommender: IPython Notebook
- Programming assignment

### Week 6: Deep Learning: Searching for Images & Closing Remarks
- Neural networks: Learning very non-linear features
- Deep learning & deep features
- Summary of deep learning
- Deep features for image classification: IPython Notebook
- Deep features for image retrieval: IPython Notebook
- Programming assignment
- Deploying machine learning as a service
- Machine learning challenges and future directions

## Course 2: [Machine Learning: Regression](https://www.coursera.org/learn/ml-regression) `25H`

### Week 1: Welcome & Simple Linear Regression
- What is this course about?
- Regression fundamentals
- The simple linear regression model, its use, and interpretation
- An aside on optimization: one dimensional objectives
- An aside on optimization: multidimensional objectives
- Finding the least squares line
- Discussion and summary of simple linear regression
- Programming assignment

### Week 2: Multiple Regression
- Multiple features of one input
- Incorporating multiple inputs
- Setting the stage for computing the least squares fit
- Computing the least squares D-dimensional curve
- Summarizing multiple regression
- Programming assignment 1
- Programming assignment 2

### Week 3: Assessing Performance
- Defining how we assess performance
- 3 measures of loss and their trends with model complexity
- 3 sources of error and the bias-variance tradeoff
- OPTIONAL ADVANCED MATERIAL: Formally defining and deriving the 3 sources of error
- Putting the pieces together
- Programming assignment

### Week 4: Ridge Regression
- Characteristics of overfit models
- The ridge objective
- Optimizing the ridge objective
- Tying up the loose ends
- Programming Assignment 1
- Programming Assignment 2

### Week 5: Feature Selection & Lasso
- Feature selection via explicit model enumeration
- Feature selection implicitly via regularized regression
- Geometric intuition for sparsity of lasso solutions
- Setting the stage for solving the lasso
- Optimizing the lasso objective
- OPTIONAL ADVANCED MATERIAL: Deriving the lasso coordinate descent update
- Tying up loose ends
- Programming Assignment 1
- Programming Assignment 2

### Week 6: Nearest Neighbors & Kernel Regression & Closing Remarks
- Motivating local fits
- Nearest neighbor regression
- k-Nearest neighbors and weighted k-nearest neighbors
- Kernel regression
- k-NN and kernel regression wrapup
- Programming Assignment
- What we've learned
- Summary and what's ahead in the specialization

## Course 3: [Machine Learning: Classification](https://www.coursera.org/learn/ml-classification) `25H`

### Week 1: Welcome! & Linear Classifiers & Logistic Regression
- Welcome to the course
- Course overview and details
- Linear classifiers
- Class probabilities
- Logistic regression
- Practical issues for classification
- Summarizing linear classifiers & logistic regression
- Programming Assignment

### Week 2: Learning Linear Classifiers & Overfitting & Regularization in Logistic Regression
- Maximum likelihood estimation
- Gradient ascent algorithm for learning logistic regression classifier
- Choosing step size for gradient ascent/descent
- (VERY OPTIONAL LESSON) Deriving gradient of logistic regression
- Summarizing learning linear classifiers
- Programming Assignment
- Overfitting in classification
- Overconfident predictions due to overfitting
- L2 regularized logistic regression
- Sparse logistic regression
- Summarizing overfitting & regularization in logistic regression
- Programming Assignment

### Week 3: Decision Trees
- Intuition behind decision trees
- Learning decision trees
- Using the learned decision tree
- Learning decision trees with continuous inputs
- Summarizing decision trees
- Programming Assignment 1
- Programming Assignment 2

### Week 4: Preventing Overfitting in Decision Trees & Handling Missing Data
- Overfitting in decision trees
- Early stopping to avoid overfitting
- (OPTIONAL LESSON) Pruning decision trees
- Summarizing preventing overfitting in decision trees
- Programming Assignment
- Basic strategies for handling missing data
- Strategy 3: Modify learning algorithm to explicitly handle missing data
- Summarizing handling missing data

### Week 5: Boosting
- The amazing idea of boosting a classifier
- AdaBoost
- Applying AdaBoost
- Programming Assignment 1
- Convergence and overfitting in boosting
- Summarizing boosting
- Programming Assignment 2

### Week 6: Precision-Recall
- Why use precision & recall as quality metrics
- Precision & recall explained
- The precision-recall tradeoff
- Summarizing precision-recall
- Programming Assignment

### Week 7: Scaling to Huge Datasets & Online Learning
- Scaling ML to huge datasets
- Scaling ML with stochastic gradient
- Understanding why stochastic gradient works
- Stochastic gradient: Practical tricks
- Online learning: Fitting models from streaming data
- Summarizing scaling to huge datasets & online learning
- Programming Assignment

## Course 4: [Machine Learning: Clustering & Retrieval](https://www.coursera.org/learn/ml-clustering-and-retrieval) `25H`

### Week 1: Welcome
- What is this course about?

### Week 2: Nearest Neighbor Search
- Introduction to nearest neighbor search and algorithms
- The importance of data representations and distance metrics
- Programming Assignment 1
- Scaling up k-NN search using KD-trees
- Locality sensitive hashing for approximate NN search
- Programming Assignment 2
- Summarizing nearest neighbor search

### Week 3: Clustering with k-means
- Introduction to clustering
- Clustering via k-means
- Programming Assignment
- MapReduce for scaling k-means
- Summarizing clustering with k-means

### Week 4: Mixture Models
- Motivating and setting the foundation for mixture models
- Mixtures of Gaussians for clustering
- Expectation Maximization (EM) building blocks
- The EM algorithm
- Summarizing mixture models
- Programming Assignment 1
- Programming Assignment 2

### Week 5: Mixed Membership Modeling via Latent Dirichlet Allocation
- Introduction to latent Dirichlet allocation
- Bayesian inference via Gibbs sampling
- Collapsed Gibbs sampling for LDA
- Summarizing latent Dirichlet allocation
- Programming Assignment

### Week 6: Hierarchical Clustering & Closing Remarks
- What we've learned
- Hierarchical clustering and clustering for time series segmentation
- Programming Assignment
- Summary and what's ahead in the specialization
